{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The purpose of this project is to model COVID-19 daily reporting data from around the world into a star shema in order to actively track the course of the pandemic. The data ingestion will be controlled by Airflow with the data residing in an Amazon Redshift MPP data warehouse. The purpose of this notebook is to get familiar with the daily covid data provided by Johns Hoplins University (https://github.com/CSSEGISandData/COVID19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/07-27-2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is data from 7-26-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good news. Alas, JHU has already done some major data cleaning.\n",
    "The field Country_Region, which is the country, doesn't appear to be null. It seems that Province States can be NULL as some countries do not have robust enough reporting to break down the data. \n",
    "\n",
    "Let's see if we can find an example of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Province_State'].isnull()].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that France is not reporting region level data in this report. Let's dig in further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Country_Region'] == 'France']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it looks like there are numerous territories under France represented here. Not a problem!\n",
    "\n",
    "Let's see how many countries are represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['Country_Region'].unique()) #188 countries represented, what are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in df['Country_Region'].unique():\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many states/provinces per country? Which one has the most?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Country_Region')['Province_State'].nunique().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, Russia beats out the US with Japan a close 3rd. Now that we have a good sense of the countries/regions represented...let's explore a little the case # data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Country_Region').sum()['Confirmed'].sort_values(ascending=False).head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Country_Region').sum()['Deaths'].sort_values(ascending=False).head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Country_Region').sum()['Recovered'].sort_values(ascending=False).head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Country_Region').mean()['Incidence_Rate'].sort_values(ascending=False).head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Country_Region').mean()['Case-Fatality_Ratio'].sort_values(ascending=False).head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we have a good sense of the World Data, lets dig into how we might want to model this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as modeling, I think there are going to be 5 main tables in our Star Schema.\n",
    "\n",
    "dim_city\n",
    "    -based on the Admin2 field, many NULLS as not every Province/Country has a corresponding \"city\"\n",
    " \n",
    "dim_state\n",
    "    - based on the Province_State field\n",
    "\n",
    "dim_country\n",
    "    - based on the Country_Region field \n",
    "\n",
    "dim_coordinates\n",
    "    - based on the lat/long coordinates of each row\n",
    "    - use the combined key to match coordinates\n",
    "\n",
    "    \n",
    "fact_covid \n",
    "    - will contain measures for confirmed, active, recovered, incidence_rate, case_fatality_ratio\n",
    "    - Will use the timestamp column last_updated and output the date as YYYY-MM-DD\n",
    "    \n",
    "All other fields will be discarded from the raw data as they are not necessary to conduct our Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation Discovery Work\n",
    "\n",
    "For the next section of the notebook, we will load a few days worth of data into a sqlite db in order to practice writing the necessary SQL for our transformations. The data flow will include a staging_raw table which is going to be an exact dump of data from the JHU github repo. So, this sqlite db will serve as a good practive environment for writing the necessary SQL statements in order to transform raw data into our star schema mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_link = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/{}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['07-24-2020','07-25-2020','07-26-2020']\n",
    "df_list = []\n",
    "for day in dates:\n",
    "    df_list.append(pd.read_csv(gh_link.format(day)))\n",
    "    \n",
    "df = pd.concat(df_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite://', echo=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('staging_raw_covid', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('SELECT * FROM staging_raw_covid LIMIT 5', con=engine) # ok, that worked, let's get to modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dim Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "\n",
    "SELECT\n",
    "    DISTINCT Country_Region AS country\n",
    "FROM staging_raw_covid\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(sql, con=engine) #great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(sql, con=engine)\n",
    "df.to_sql('dim_country', con=engine, index_label='country_id', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('SELECT * FROM dim_country', con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dim State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "\n",
    "SELECT\n",
    "    DISTINCT Province_State AS state\n",
    "FROM staging_raw_covid \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(sql, con=engine) #great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(sql, con=engine)\n",
    "df.to_sql('dim_state', con=engine,index_label='state_id', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('SELECT * FROM dim_state', con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "\n",
    "SELECT \n",
    "    DISTINCT Admin2 AS city\n",
    "FROM staging_raw_covid\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(sql, con=engine) #great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(sql, con=engine)\n",
    "df.to_sql('dim_city', con=engine, index_label='city_id', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('SELECT * FROM dim_city', con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "\n",
    "SELECT\n",
    "   DISTINCT Combined_Key AS location,\n",
    "     Lat AS latitude,\n",
    "     Long_ AS longitude\n",
    "FROM staging_raw_covid\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(sql, con=engine) #great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(sql, con=engine)\n",
    "df.to_sql('dim_coordinates', con=engine, if_exists='replace', index=True, index_label = 'coordinate_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('SELECT * FROM dim_coordinates', con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query('SELECT * FROM staging_raw_covid', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df['Last_Update'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "\n",
    "SELECT\n",
    "    DATE(s.Last_Update) AS date,\n",
    "    ci.city_id,\n",
    "    st.state_id,\n",
    "    c.country_id,\n",
    "    coor.coordinate_id,\n",
    "    s.Confirmed,\n",
    "    s.Deaths,\n",
    "    s.Recovered,\n",
    "    s.Active,\n",
    "    s.Incidence_Rate\n",
    "FROM staging_raw_covid s\n",
    "LEFT JOIN dim_country c ON c.country=s.Country_Region\n",
    "LEFT JOIN dim_state st ON st.state=s.Province_State\n",
    "LEFT JOIN dim_city ci ON ci.city=s.Admin2\n",
    "LEFT JOIN dim_coordinates coor ON coor.location=s.Combined_key\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(sql, con=engine) #great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a good idea of how to model our data. The actual SQL used to create the star schema may be a little different as we had to imporovise in our sqlite/pandas environment, however the basic ideas will remain the same!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
